{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef70d67-1891-4943-9592-7f11c072078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data analysis\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./data/Cleaned_S_P_500_Data.csv', delimiter=',', index_col=False, header=0)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import graphviz\n",
    "import lingam\n",
    "from lingam.utils import make_dot, print_causal_directions, print_dagc\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import statistics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "id": "aadb91f3-6214-47a3-9a29-d01d0f1b884a",
   "metadata": {},
   "source": [
    "# first_200_columns = data.iloc[:, :200]\n",
    "model = lingam.VARLiNGAM(lags=3)\n",
    "model.fit(data)\n",
    "\n",
    "# print(len(model.adjacency_matrices_))\n",
    "df = pd.DataFrame(model.adjacency_matrices_[2])\n",
    "df.to_csv('adjacencymatrix2.csv', index=False, header=False)\n",
    "\n",
    "summary_matrix = np.sum(model.adjacency_matrices_, axis=0)\n",
    "\n",
    "model = lingam.VARLiNGAM(lags=5)\n",
    "model.fit(new_data)\n",
    "# summary_matrix = np.sum(model.adjacency_matrices_, axis=0)\n",
    "\n",
    "first_10_columns = data.iloc[:, :50]\n",
    "model = lingam.VARLiNGAM(lags=1)\n",
    "model.fit(first_10_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a60e7b-7777-4d94-8a21-08528365c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/Cleaned_S_P_500_Data.csv', delimiter=',', index_col=False, header=0)\n",
    "predictions = pd.read_csv('./data/sp500_predictions.csv', delimiter=',', index_col=False, header=0)\n",
    "\n",
    "predictions.columns = data.columns\n",
    "print(predictions)\n",
    "\n",
    "data_backtest = data.tail(len(predictions)+1).reset_index(drop=True)\n",
    "print(data_backtest)\n",
    "\n",
    "predicted_returns = (predictions-data_backtest[:-1])/data_backtest[:-1]\n",
    "print(predicted_returns)\n",
    "\n",
    "real_returns = data_backtest.pct_change()\n",
    "print(real_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3aac5-e613-45d6-a672-e2dfaf9cd1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_returns = (data.shift(-3) - data)/3\n",
    "predicted_returns = predicted_returns.dropna().iloc[:-1]\n",
    "predicted_returns = predicted_returns.tail(len(predictions)).reset_index(drop=True)\n",
    "predicted_returns\n",
    "\n",
    "# for i in range(len(predicted_returns)):\n",
    "#     print(i)\n",
    "\n",
    "predicted_returns.loc[4]\n",
    "winners = predicted_returns.loc[4].nlargest(10).index\n",
    "print(winners)\n",
    "try_data = real_returns.loc[5, winners].mean()\n",
    "type(try_data)\n",
    "\n",
    "def calculate_cumulative_portfolio_returns(fraction):\n",
    "    num_stocks = len(data.columns)\n",
    "    num_winners = int(num_stocks * fraction)\n",
    "    portfolio_returns = []\n",
    "\n",
    "    for i in range(len(predicted_returns)):\n",
    "        winners = predicted_returns.loc[i].nlargest(num_winners).index\n",
    "        losers = predicted_returns.loc[i].nsmallest(num_winners).index\n",
    "        winner_return = real_returns.loc[i+1, winners].mean()\n",
    "        loser_return = real_returns.loc[i+1, losers].mean()\n",
    "        portfolio_return = winner_return - loser_return\n",
    "        portfolio_returns.append(portfolio_return)\n",
    "        \n",
    "    return np.exp(np.sum(np.log(np.array(portfolio_returns)+1)))-1\n",
    "\n",
    "# returns_series = pd.Series(portfolio_returns)\n",
    "# cumulative_portfolio_returns = (1 + returns_series).cumprod() - 1\n",
    "calculate_cumulative_portfolio_returns(0.1)\n",
    "\n",
    "frac = np.arange(0.01, 0.5, step=0.01)\n",
    "cpr = [calculate_cumulative_portfolio_returns(f) for f in frac]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(frac, cpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212bf2ce-31cb-487b-9d10-51fab881c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging code for networkx\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "matrix0 = pd.read_csv('./varlingam_causal_structure/adjacencymatrix0.csv', header=None)\n",
    "matrix1 = pd.read_csv('./varlingam_causal_structure/adjacencymatrix1.csv', header=None)\n",
    "summary_matrix = np.abs(matrix0) + np.abs(matrix1)\n",
    "summary_matrix\n",
    "\n",
    "nonzero_columns = summary_matrix.apply(lambda row: row[row != 0].index.tolist(), axis=1)\n",
    "print(nonzero_columns[0])\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "causal_graph_filename = \"causal_graphs/sp500_graph_varlingam_lag_3.txt\"\n",
    "G = nx.convert_node_labels_to_integers(nx.read_adjlist(causal_graph_filename, create_using=nx.DiGraph))\n",
    "causes_index = list(G.predecessors(0))\n",
    "causes_index\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./data/Cleaned_CSI_300_Data.csv', delimiter=',', index_col=False, header=0)\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import graphviz\n",
    "import lingam\n",
    "from lingam.utils import make_dot, print_causal_directions, print_dagc\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import statistics\n",
    "\n",
    "model = lingam.VARLiNGAM(lags=1)\n",
    "model.fit(data)\n",
    "\n",
    "summary_matrix = np.sum(np.abs(model.adjacency_matrices_), axis=0)\n",
    "nonzero_columns = np.nonzero(summary_matrix[0, :])[0]\n",
    "print(nonzero_columns)\n",
    "\n",
    "causal_graph_filename = \"causal_graphs/csi300_graph_varlingam_lag_1.txt\"\n",
    "# G = nx.convert_node_labels_to_integers(nx.read_adjlist(causal_graph_filename, create_using=nx.DiGraph))\n",
    "G = nx.read_adjlist(causal_graph_filename, create_using=nx.DiGraph)\n",
    "edges = list(G.edges)\n",
    "# print(edges)\n",
    "stock_index = 0\n",
    "causes_index = list(G.predecessors(str(stock_index)))\n",
    "causes_index\n",
    "int_list = [int(item) for item in causes_index]\n",
    "int_list\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "summary_matrix = np.eye(30)\n",
    "G_self = nx.from_numpy_array(summary_matrix.T, create_using=nx.DiGraph)\n",
    "for u, v, d in G_self.edges(data=True):\n",
    "    del d['weight']\n",
    "G_self = nx.relabel_nodes(G_self, lambda x: str(x))\n",
    "edges_self = list(G_self.edges)\n",
    "print(edges_self)\n",
    "stock_index = 0\n",
    "causes_self_index = list(G_self.predecessors(str(stock_index)))\n",
    "causes_self_index\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./data/Cleaned_S_P_500_Data.csv', delimiter=',', index_col=False, header=0)\n",
    "\n",
    "A = data.iloc[:, 0]\n",
    "A = A.to_numpy()\n",
    "A\n",
    "\n",
    "A_lag = A[1:]\n",
    "A_lag\n",
    "\n",
    "A_lag2 = A_lag[1:]\n",
    "A_lag2\n",
    "\n",
    "X = np.column_stack((A_lag[:-1], A_lag2))\n",
    "X\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X[:-1,:], A[:-3].reshape(-1, 1))\n",
    "prediction = model.predict(X[-1,:].reshape(1, -1))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b085617-421a-4776-ad4e-53a7c303fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VarLiNGAM interface prototype\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import lingam\n",
    "from lingam.utils import make_dot, print_causal_directions, print_dagc\n",
    "\n",
    "def causal_discovery_varlingam(data, lag):\n",
    "    model = lingam.VARLiNGAM(lags=lag)\n",
    "    model.fit(data)\n",
    "    summary_matrix = np.sum(np.abs(model.adjacency_matrices_), axis=0)\n",
    "    causal_graph = nx.from_numpy_array(summary_matrix.T, create_using=nx.DiGraph)\n",
    "    for u, v, d in causal_graph.edges(data=True):\n",
    "            del d['weight']\n",
    "    return causal_graph\n",
    "\n",
    "def causal_discovery(data, lag_range, market_name, algorithm):\n",
    "    directory = \"./causal_graph\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    for i in lag_range:\n",
    "        print(f\"lag={i}\")\n",
    "        if algorithm == \"varlingam\":\n",
    "            G = causal_discovery_varlingam(data, i)\n",
    "        filename = os.path.join(directory, f'{market_name}_graph_{algorithm}_lag_{i}.txt')\n",
    "        nx.write_adjlist(G, filename)\n",
    "\n",
    "data = pd.read_csv('./data/Cleaned_S_P_500_Data.csv', delimiter=',', index_col=False, header=0)\n",
    "data = data.to_numpy()\n",
    "\n",
    "causal_discovery(data[:,:10], range(1,7), \"sp500\", \"varlingam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4033644-4244-43ee-8bc0-2e82c06b198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch test prototype\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import lingam\n",
    "from lingam.utils import make_dot, print_causal_directions, print_dagc\n",
    "\n",
    "def causal_discovery_varlingam(data, lag):\n",
    "    model = lingam.VARLiNGAM(lags=lag)\n",
    "    model.fit(data)\n",
    "    summary_matrix = np.sum(np.abs(model.adjacency_matrices_), axis=0)\n",
    "    causal_graph = nx.from_numpy_array(summary_matrix.T, create_using=nx.DiGraph)\n",
    "    for u, v, d in causal_graph.edges(data=True):\n",
    "            del d['weight']\n",
    "    return causal_graph\n",
    "\n",
    "def causal_discovery(data, lag_range, market_name, algorithm):\n",
    "    directory = \"./causal_graph\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    for i in lag_range:\n",
    "        print(f\"lag={i}\")\n",
    "        if algorithm == \"varlingam\":\n",
    "            G = causal_discovery_varlingam(data, i)\n",
    "        filename = os.path.join(directory, f'{market_name}_graph_{algorithm}_lag_{i}.txt')\n",
    "        nx.write_adjlist(G, filename)\n",
    "\n",
    "data = pd.read_csv('./data/Cleaned_S_P_500_Data.csv', delimiter=',', index_col=False, header=0)\n",
    "data = data.to_numpy()\n",
    "causal_discovery(data, range(1,6), \"sp500\", \"varlingam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6481a7ab-f5ce-440f-b619-82073b2e30a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction util prototype\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def predict_single(data, lag, G, stock_index, train_frac=0.8):\n",
    "    predicted = []\n",
    "    causes_index = list(G.predecessors(stock_index))\n",
    "    causes = data[:, causes_index]\n",
    "    target = data[:, stock_index]\n",
    "    Y = target[lag:]\n",
    "    X = np.empty((0, lag*len(causes_index)))\n",
    "    train_length = int(len(Y)*train_frac)\n",
    "    \n",
    "    for i in range(lag, data.shape[0]):\n",
    "        lagged_vars = causes[(i-lag):i, :]\n",
    "        long_vars = np.concatenate(lagged_vars)\n",
    "        X = np.vstack([X, long_vars])\n",
    "    \n",
    "    for t in range(train_length, len(Y)):\n",
    "        model = LinearRegression()\n",
    "        model.fit(X[:t, :], Y[:t])\n",
    "        prediction = model.predict(X[t, :].reshape(1, -1))\n",
    "        predicted.append(prediction[0])\n",
    "    return predicted\n",
    "\n",
    "def predict_batch(data, lag, G, train_frac = 0.8):\n",
    "    predictions = np.empty((0, 0))\n",
    "    for i in range(data.shape[1]):\n",
    "        print(f\"Predicting stock {i}\")\n",
    "        predicted_list = predict_single(data, lag, G, i, train_frac)\n",
    "        new_column = np.array(predicted_list).reshape(-1, 1)\n",
    "        predictions = np.hstack((predictions, new_column)) if predictions.size else new_column\n",
    "    return predictions\n",
    "\n",
    "def calculate_cumulative_portfolio_returns(data, predictions, winner_frac):\n",
    "    data_backtest = data[-(predictions.shape[0]+1):]\n",
    "    predicted_returns = (predictions - data_backtest[:-1]) / data_backtest[:-1]\n",
    "    real_returns = (data_backtest[1:] - data_backtest[:-1]) / data_backtest[:-1]\n",
    "    num_stocks = data.shape[1]\n",
    "    num_winners = int(num_stocks * winner_frac)\n",
    "    portfolio_returns = []\n",
    "    winners = np.argpartition(predicted_returns, -num_winners, axis=1)[:, -num_winners:]\n",
    "    losers = np.argpartition(predicted_returns, num_winners, axis=1)[:, :num_winners]\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        winner_return = np.mean(real_returns[i, winners[i]])\n",
    "        loser_return = np.mean(real_returns[i, losers[i]])\n",
    "        portfolio_return = winner_return - loser_return\n",
    "        portfolio_returns.append(portfolio_return)\n",
    "        \n",
    "    return np.exp(np.sum(np.log(np.array(portfolio_returns)+1)))-1\n",
    "\n",
    "# predict\n",
    "G = nx.convert_node_labels_to_integers(nx.read_adjlist('./causal_graph/sp500_graph_varlingam_lag_3.txt', create_using=nx.DiGraph))\n",
    "data = pd.read_csv('./data/Cleaned_S_P_500_Data.csv', delimiter=',', index_col=False, header=0)\n",
    "column_names = data.columns.tolist()\n",
    "data = data.to_numpy()\n",
    "predictions = predict_batch(data, 3, G)\n",
    "np.savetxt(\"./predictions/sp500_predictions_varlingam_lag_3.csv\", predictions, delimiter=\",\", header=\",\".join(column_names), comments='')\n",
    "\n",
    "# backtest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "frac = np.arange(0.01, 0.5, step=0.01)\n",
    "cpr = [calculate_cumulative_portfolio_returns(data, predictions, f) for f in frac]\n",
    "\n",
    "plt.plot(frac, cpr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
